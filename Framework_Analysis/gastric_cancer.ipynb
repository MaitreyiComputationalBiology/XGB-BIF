{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOBTvnwBdALs"
      },
      "outputs": [],
      "source": [
        "#Importing all the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, roc_auc_score, cohen_kappa_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.feature_selection import mutual_info_classif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-M5mlgUcktG"
      },
      "outputs": [],
      "source": [
        "#Loading the dataset\n",
        "df = pd.read_csv(\"trial_dataframe.csv\")\n",
        "\n",
        "#Encoding the target variable\n",
        "le = LabelEncoder()\n",
        "df[\"Sample_Characteristics\"] = le.fit_transform(df[\"Sample_Characteristics\"])\n",
        "\n",
        "#Defining features and target\n",
        "X = df.drop(columns=[\"Sample_Characteristics\"])\n",
        "y = df[\"Sample_Characteristics\"]\n",
        "\n",
        "# Splitting dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "pS5ez_lFysdx",
        "outputId": "77312412-e0b3-4fc2-c859-b507841bf57c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-33285216-764c-4db4-aff2-ab785801bed5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TSPAN6</th>\n",
              "      <th>TNMD</th>\n",
              "      <th>DPM1</th>\n",
              "      <th>SCYL3</th>\n",
              "      <th>FIRRM</th>\n",
              "      <th>FGR</th>\n",
              "      <th>CFH</th>\n",
              "      <th>FUCA2</th>\n",
              "      <th>GCLC</th>\n",
              "      <th>NFYA</th>\n",
              "      <th>...</th>\n",
              "      <th>LOC105370174</th>\n",
              "      <th>C8orf44-SGK3</th>\n",
              "      <th>SNORA74C-2</th>\n",
              "      <th>ELOA3BP</th>\n",
              "      <th>NPBWR1</th>\n",
              "      <th>ELOA3DP</th>\n",
              "      <th>LNCDAT</th>\n",
              "      <th>LOC124902537</th>\n",
              "      <th>RNF228</th>\n",
              "      <th>PANO1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12.127672</td>\n",
              "      <td>3.321928</td>\n",
              "      <td>10.802516</td>\n",
              "      <td>10.412570</td>\n",
              "      <td>8.761551</td>\n",
              "      <td>9.859535</td>\n",
              "      <td>11.256209</td>\n",
              "      <td>12.131857</td>\n",
              "      <td>11.154185</td>\n",
              "      <td>11.294621</td>\n",
              "      <td>...</td>\n",
              "      <td>4.459432</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.459432</td>\n",
              "      <td>4.459432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11.796040</td>\n",
              "      <td>4.954196</td>\n",
              "      <td>11.309476</td>\n",
              "      <td>10.865733</td>\n",
              "      <td>9.799282</td>\n",
              "      <td>9.475733</td>\n",
              "      <td>12.087794</td>\n",
              "      <td>11.087463</td>\n",
              "      <td>11.272047</td>\n",
              "      <td>11.890644</td>\n",
              "      <td>...</td>\n",
              "      <td>3.584963</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.584963</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.169925</td>\n",
              "      <td>4.857981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9.515700</td>\n",
              "      <td>6.832890</td>\n",
              "      <td>12.538189</td>\n",
              "      <td>11.606868</td>\n",
              "      <td>10.055282</td>\n",
              "      <td>8.848623</td>\n",
              "      <td>12.170551</td>\n",
              "      <td>12.151017</td>\n",
              "      <td>11.145932</td>\n",
              "      <td>12.099677</td>\n",
              "      <td>...</td>\n",
              "      <td>5.285402</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.066089</td>\n",
              "      <td>5.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.951649</td>\n",
              "      <td>5.672425</td>\n",
              "      <td>11.074141</td>\n",
              "      <td>10.405141</td>\n",
              "      <td>8.891784</td>\n",
              "      <td>7.894818</td>\n",
              "      <td>11.253257</td>\n",
              "      <td>11.149112</td>\n",
              "      <td>11.365229</td>\n",
              "      <td>10.903129</td>\n",
              "      <td>...</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.584963</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.584963</td>\n",
              "      <td>5.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11.905011</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>11.626622</td>\n",
              "      <td>10.463524</td>\n",
              "      <td>9.317413</td>\n",
              "      <td>8.873444</td>\n",
              "      <td>12.127027</td>\n",
              "      <td>11.338179</td>\n",
              "      <td>10.645658</td>\n",
              "      <td>10.631177</td>\n",
              "      <td>...</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.807355</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.169925</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.807355</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 31574 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-33285216-764c-4db4-aff2-ab785801bed5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-33285216-764c-4db4-aff2-ab785801bed5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-33285216-764c-4db4-aff2-ab785801bed5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a8829479-c8a1-4e66-845d-dc2b1f176caa\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a8829479-c8a1-4e66-845d-dc2b1f176caa')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a8829479-c8a1-4e66-845d-dc2b1f176caa button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "      TSPAN6      TNMD       DPM1      SCYL3      FIRRM       FGR        CFH  \\\n",
              "0  12.127672  3.321928  10.802516  10.412570   8.761551  9.859535  11.256209   \n",
              "1  11.796040  4.954196  11.309476  10.865733   9.799282  9.475733  12.087794   \n",
              "2   9.515700  6.832890  12.538189  11.606868  10.055282  8.848623  12.170551   \n",
              "3  11.951649  5.672425  11.074141  10.405141   8.891784  7.894818  11.253257   \n",
              "4  11.905011  0.000000  11.626622  10.463524   9.317413  8.873444  12.127027   \n",
              "\n",
              "       FUCA2       GCLC       NFYA  ...  LOC105370174  C8orf44-SGK3  \\\n",
              "0  12.131857  11.154185  11.294621  ...      4.459432           0.0   \n",
              "1  11.087463  11.272047  11.890644  ...      3.584963           0.0   \n",
              "2  12.151017  11.145932  12.099677  ...      5.285402           0.0   \n",
              "3  11.149112  11.365229  10.903129  ...      4.000000           1.0   \n",
              "4  11.338179  10.645658  10.631177  ...      3.000000           0.0   \n",
              "\n",
              "   SNORA74C-2  ELOA3BP    NPBWR1  ELOA3DP    LNCDAT  LOC124902537    RNF228  \\\n",
              "0         1.0      0.0  0.000000      0.0  0.000000           0.0  3.459432   \n",
              "1         1.0      0.0  1.584963      0.0  0.000000           0.0  3.169925   \n",
              "2         0.0      0.0  1.000000      0.0  0.000000           0.0  7.066089   \n",
              "3         0.0      0.0  1.584963      0.0  0.000000           0.0  2.584963   \n",
              "4         0.0      0.0  3.807355      0.0  3.169925           0.0  1.000000   \n",
              "\n",
              "      PANO1  \n",
              "0  4.459432  \n",
              "1  4.857981  \n",
              "2  5.000000  \n",
              "3  5.000000  \n",
              "4  3.807355  \n",
              "\n",
              "[5 rows x 31574 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Checking in with the dataset\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OHZ22TFV245",
        "outputId": "7165eebf-815a-44da-8ea3-c6ea5149842a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9755\n",
            "Precision: 0.9032\n",
            "AUC: 0.9950\n",
            "Kappa: 0.8482\n"
          ]
        }
      ],
      "source": [
        "#No feature selection + RF\n",
        "#Initializing and training the Random Forest classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "#Making predictions\n",
        "y_pred = rf.predict(X_test)\n",
        "y_pred_proba = rf.predict_proba(X_test)[:, 1]  \n",
        "\n",
        "#Evaluating model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "#Printing evaluation metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"AUC: {auc:.4f}\")\n",
        "print(f\"Kappa: {kappa:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQMj4UKXc5Un",
        "outputId": "7e447baf-63a4-4a28-b6bd-1ecf39e21be7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9891\n",
            "Precision: 0.9688\n",
            "AUC: 0.9954\n",
            "Kappa: 0.9334\n"
          ]
        }
      ],
      "source": [
        "# No feature selection + XGB\n",
        "# Initializing and train the XGBoost classifier\n",
        "xgb_clf = xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric=\"logloss\")\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions\n",
        "y_pred = xgb_clf.predict(X_test)\n",
        "y_pred_proba = xgb_clf.predict_proba(X_test)[:, 1]  # Probability estimates for AUC calculation\n",
        "\n",
        "# Evaluating model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "# Printing evaluation metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"AUC: {auc:.4f}\")\n",
        "print(f\"Kappa: {kappa:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cF3YSxAeNhZ",
        "outputId": "42f13149-e556-411d-86d0-7434616bdf23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9864\n",
            "Precision: 0.9143\n",
            "AUC: 0.9975\n",
            "Kappa: 0.9200\n"
          ]
        }
      ],
      "source": [
        "#No feature selection + LR\n",
        "#Initializing and training the Logistic Regression model\n",
        "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "#Making predictions\n",
        "y_pred = lr.predict(X_test)\n",
        "y_pred_proba = lr.predict_proba(X_test)[:, 1]\n",
        "\n",
        "#Evaluating model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "#Printing evaluation metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"AUC: {auc:.4f}\")\n",
        "print(f\"Kappa: {kappa:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJ3QIC9-f793",
        "outputId": "a7068341-4d00-4c73-e3e3-ee5bad959064"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9864\n",
            "Precision: 0.9394\n",
            "AUC: 0.9974\n",
            "Kappa: 0.9179\n"
          ]
        }
      ],
      "source": [
        "#No feature selection + SVM\n",
        "# Initializing and training the SVM model (with probability=True for AUC calculation)\n",
        "svm_clf = SVC(kernel='rbf', probability=True, random_state=42)\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions\n",
        "y_pred = svm_clf.predict(X_test)\n",
        "y_pred_proba = svm_clf.predict_proba(X_test)[:, 1]  # Probability estimates for AUC calculation\n",
        "\n",
        "# Evaluating model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "# Printing evaluation metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"AUC: {auc:.4f}\")\n",
        "print(f\"Kappa: {kappa:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOsPgzCghpTP",
        "outputId": "a01fd384-d122-43fb-8c0e-c3114d243cb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Results for Top 10 Features:\n",
            "Accuracy: 0.8925\n",
            "Precision: 0.8462\n",
            "AUC: 0.9644\n",
            "Kappa: 0.7852\n",
            "\n",
            "Results for Top 50 Features:\n",
            "Accuracy: 0.9140\n",
            "Precision: 0.8958\n",
            "AUC: 0.9681\n",
            "Kappa: 0.8280\n",
            "\n",
            "Results for Top 100 Features:\n",
            "Accuracy: 0.9247\n",
            "Precision: 0.8980\n",
            "AUC: 0.9685\n",
            "Kappa: 0.8495\n",
            "\n",
            "Results for Top 500 Features:\n",
            "Accuracy: 0.9247\n",
            "Precision: 0.8980\n",
            "AUC: 0.9648\n",
            "Kappa: 0.8495\n",
            "\n",
            "Results for Top 1000 Features:\n",
            "Accuracy: 0.9140\n",
            "Precision: 0.8800\n",
            "AUC: 0.9681\n",
            "Kappa: 0.8281\n"
          ]
        }
      ],
      "source": [
        "#RF + SVM\n",
        "# Feature selection using Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Getting feature importances\n",
        "feature_importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
        "feature_counts = [10, 50, 100, 500, 1000]\n",
        "\n",
        "# Looping through different feature counts\n",
        "for num_features in feature_counts:\n",
        "    top_features = feature_importances.nlargest(num_features).index.tolist()\n",
        "\n",
        "    X_train_selected = X_train[top_features]\n",
        "    X_test_selected = X_test[top_features]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_selected)\n",
        "    X_test_scaled = scaler.transform(X_test_selected)\n",
        "\n",
        "    svm_clf = SVC(kernel='rbf', probability=True, random_state=42)\n",
        "    svm_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "    y_pred = svm_clf.predict(X_test_scaled)\n",
        "    y_pred_proba = svm_clf.predict_proba(X_test_scaled)[:, 1]  \n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\nResults for Top {num_features} Features:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"AUC: {auc:.4f}\")\n",
        "    print(f\"Kappa: {kappa:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsEh4OXRj17C",
        "outputId": "4f291765-475e-46e0-9a34-dad13d9934b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-71-21cdacf3c988>:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  results_df = pd.concat([results_df, pd.DataFrame([[num_features, auc, accuracy, precision, kappa]],\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Features       AUC  Accuracy  Precision     Kappa\n",
            "0       10  0.962535  0.892473   0.860000  0.785120\n",
            "1       50  0.962997  0.946237   0.936170  0.892486\n",
            "2      100  0.961147  0.935484   0.916667  0.871012\n",
            "3      500  0.981036  0.935484   0.916667  0.871012\n",
            "4     1000  0.976873  0.924731   0.882353  0.849619\n"
          ]
        }
      ],
      "source": [
        "#RF + LR\n",
        "# Feature selection using Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=500, max_depth=None, min_samples_split=2, random_state=42, n_jobs=-1)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Getting feature importances\n",
        "feature_importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
        "feature_counts = [10, 50, 100, 500, 1000]\n",
        "\n",
        "# Initializing a DataFrame to store results\n",
        "results_df = pd.DataFrame(columns=[\"Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])\n",
        "\n",
        "# Looping through different feature counts\n",
        "for num_features in feature_counts:\n",
        "    top_features = feature_importances.nlargest(num_features).index.tolist()\n",
        "\n",
        "    X_train_selected = X_train[top_features]\n",
        "    X_test_selected = X_test[top_features]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_selected)\n",
        "    X_test_scaled = scaler.transform(X_test_selected)\n",
        "\n",
        "    lr_clf = LogisticRegression(solver=\"liblinear\", C=1.0, max_iter=500, random_state=42)\n",
        "    lr_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "    y_pred = lr_clf.predict(X_test_scaled)\n",
        "    y_pred_proba = lr_clf.predict_proba(X_test_scaled)[:, 1]  \n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([[num_features, auc, accuracy, precision, kappa]],\n",
        "                                                      columns=[\"Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])],\n",
        "                                                      ignore_index=True)\n",
        "\n",
        "# Displaying final results\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkCULpFKlBH1",
        "outputId": "c142092d-5ce4-4aae-93b3-8f6ec2e5d7a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-26-ac6950e7d264>:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  results_df = pd.concat([results_df, pd.DataFrame([[num_features, auc, accuracy, precision, kappa]],\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Features       AUC  Accuracy  Precision     Kappa\n",
            "0       10  0.980342  0.946237   0.936170  0.892486\n",
            "1       50  0.971323  0.935484   0.934783  0.870953\n",
            "2      100  0.971554  0.935484   0.934783  0.870953\n",
            "3      500  0.981499  0.956989   0.956522  0.913969\n",
            "4     1000  0.978030  0.946237   0.936170  0.892486\n"
          ]
        }
      ],
      "source": [
        "#RF + RF\n",
        "# Feature selection using Random Forest\n",
        "rf_selector = RandomForestClassifier(n_estimators=500, max_depth=None, min_samples_split=2, random_state=42, n_jobs=-1)\n",
        "rf_selector.fit(X_train, y_train)\n",
        "\n",
        "# Getting feature importances\n",
        "feature_importances = pd.Series(rf_selector.feature_importances_, index=X.columns)\n",
        "feature_counts = [10, 50, 100, 500, 1000]\n",
        "\n",
        "# Initializing a DataFrame to store results\n",
        "results_df = pd.DataFrame(columns=[\"Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])\n",
        "\n",
        "# Looping through different feature counts\n",
        "for num_features in feature_counts:\n",
        "    top_features = feature_importances.nlargest(num_features).index.tolist()\n",
        "\n",
        "    X_train_selected = X_train[top_features]\n",
        "    X_test_selected = X_test[top_features]\n",
        "\n",
        "    rf_clf = RandomForestClassifier(\n",
        "        n_estimators=500,  \n",
        "        max_depth=None,    \n",
        "        min_samples_split=2, \n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    rf_clf.fit(X_train_selected, y_train)\n",
        "\n",
        "    y_pred = rf_clf.predict(X_test_selected)\n",
        "    y_pred_proba = rf_clf.predict_proba(X_test_selected)[:, 1]  \n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([[num_features, auc, accuracy, precision, kappa]],\n",
        "                                                      columns=[\"Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])],\n",
        "                                                      ignore_index=True)\n",
        "\n",
        "# Displaying final results\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dB9fr2CmXwX",
        "outputId": "6980bdd7-105f-4daf-9d88-46ea3aea5daa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:59:47] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "<ipython-input-6-24ce7b41a5b5>:53: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  results_df = pd.concat([results_df, pd.DataFrame([[num_features, auc, accuracy, precision, kappa]],\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:59:47] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:59:49] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:59:51] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:59:54] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Features       AUC  Accuracy  Precision     Kappa\n",
            "0       10  0.974534  0.920863   0.914286  0.841735\n",
            "1       50  0.977847  0.920863   0.914286  0.841735\n",
            "2      100  0.980538  0.920863   0.902778  0.841768\n",
            "3      500  0.991097  0.964029   0.970588  0.928046\n",
            "4     1000  0.989855  0.942446   0.942029  0.884886\n"
          ]
        }
      ],
      "source": [
        "#RF + XGB\n",
        "# Feature selection using Random Forest\n",
        "rf_selector = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=2, random_state=42, n_jobs=-1)\n",
        "rf_selector.fit(X_train, y_train)\n",
        "\n",
        "# Getting feature importances\n",
        "feature_importances = pd.Series(rf_selector.feature_importances_, index=X.columns)\n",
        "feature_counts = [10, 50, 100, 500, 1000]\n",
        "\n",
        "# Initializing a DataFrame to store results\n",
        "results_df = pd.DataFrame(columns=[\"Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])\n",
        "\n",
        "# Looping through different feature counts\n",
        "for num_features in feature_counts:\n",
        "    top_features = feature_importances.nlargest(num_features).index.tolist()\n",
        "\n",
        "    X_train_selected = X_train[top_features]\n",
        "    X_test_selected = X_test[top_features]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_selected)\n",
        "    X_test_scaled = scaler.transform(X_test_selected)\n",
        "\n",
        "    xgb_clf = XGBClassifier(\n",
        "        n_estimators=500,  \n",
        "        learning_rate=0.05,  \n",
        "        max_depth=6,  \n",
        "        subsample=0.8,  \n",
        "        colsample_bytree=0.8, \n",
        "        random_state=42,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric=\"logloss\"\n",
        "    )\n",
        "    xgb_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "    y_pred = xgb_clf.predict(X_test_scaled)\n",
        "    y_pred_proba = xgb_clf.predict_proba(X_test_scaled)[:, 1]  \n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([[num_features, auc, accuracy, precision, kappa]],\n",
        "                                                      columns=[\"Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])],\n",
        "                                                      ignore_index=True)\n",
        "\n",
        "# Displaying final results\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-K0hIm_ioNLj",
        "outputId": "40ff91fe-8a8f-434c-a76b-5642b00f89cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [20:16:49] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "<ipython-input-10-551f7be253fd>:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  results_df = pd.concat([results_df, pd.DataFrame([[num_features, auc, accuracy, precision, kappa]],\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Features       AUC  Accuracy  Precision     Kappa\n",
            "0       10  0.994372  0.994083     0.9375  0.964488\n",
            "1       50  0.994372  0.994083     0.9375  0.964488\n",
            "2      100  0.996104  0.994083     0.9375  0.964488\n",
            "3      500  0.996104  0.994083     0.9375  0.964488\n",
            "4     1000  0.995671  0.994083     0.9375  0.964488\n"
          ]
        }
      ],
      "source": [
        "#XGB + SVM\n",
        "# Training XGBoost to get feature importances\n",
        "xgb_selector = XGBClassifier(n_estimators=500, learning_rate=0.05, max_depth=6,\n",
        "                             subsample=0.8, colsample_bytree=0.8, random_state=42,\n",
        "                             use_label_encoder=False, eval_metric=\"logloss\")\n",
        "\n",
        "xgb_selector.fit(X_train, y_train)\n",
        "\n",
        "# Getting feature importances\n",
        "feature_importances = pd.Series(xgb_selector.feature_importances_, index=X.columns)\n",
        "feature_counts = [10, 50, 100, 500, 1000]\n",
        "\n",
        "# Initializing a DataFrame to store results\n",
        "results_df = pd.DataFrame(columns=[\"Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])\n",
        "\n",
        "# Looping through different feature counts\n",
        "for num_features in feature_counts:\n",
        "    top_features = feature_importances.nlargest(num_features).index.tolist()\n",
        "\n",
        "    X_train_selected = X_train[top_features]\n",
        "    X_test_selected = X_test[top_features]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_selected)\n",
        "    X_test_scaled = scaler.transform(X_test_selected)\n",
        "\n",
        "    svm_clf = SVC(kernel=\"rbf\", C=1.0, probability=True, random_state=42)\n",
        "    svm_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "    y_pred = svm_clf.predict(X_test_scaled)\n",
        "    y_pred_proba = svm_clf.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([[num_features, auc, accuracy, precision, kappa]],\n",
        "                                                      columns=[\"Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])],\n",
        "                                                      ignore_index=True)\n",
        "\n",
        "# Displaying final results\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isQCDy8Eo2DF",
        "outputId": "e9804ccf-0ae8-45a1-da3a-216b344ec85e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:15:01] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "<ipython-input-35-e63006ebf23e>:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  results_df = pd.concat([results_df, pd.DataFrame([[num_features, auc, accuracy, precision, kappa]],\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Features       AUC  Accuracy  Precision     Kappa\n",
            "0       10  0.998151  0.991848   0.942857  0.952025\n",
            "1       50  0.998151  0.989130   0.941176  0.935188\n",
            "2      100  0.998503  0.991848   0.942857  0.952025\n",
            "3      500  0.998503  0.991848   0.918919  0.953244\n",
            "4     1000  0.998503  0.991848   0.918919  0.953244\n"
          ]
        }
      ],
      "source": [
        "#XGB + LR\n",
        "# Training XGBoost to get feature importances\n",
        "xgb_selector = XGBClassifier(n_estimators=500, learning_rate=0.05, max_depth=6,\n",
        "                             subsample=0.8, colsample_bytree=0.8, random_state=42,\n",
        "                             use_label_encoder=False, eval_metric=\"logloss\")\n",
        "\n",
        "xgb_selector.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = pd.Series(xgb_selector.feature_importances_, index=X.columns)\n",
        "\n",
        "# List of feature selection counts\n",
        "feature_counts = [10, 50, 100, 500, 1000]\n",
        "\n",
        "# Initialize a DataFrame to store results\n",
        "results_df = pd.DataFrame(columns=[\"Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])\n",
        "\n",
        "# Loop through different feature counts\n",
        "for num_features in feature_counts:\n",
        "    # Select top N features\n",
        "    top_features = feature_importances.nlargest(num_features).index.tolist()\n",
        "\n",
        "    # Filter dataset with selected features\n",
        "    X_train_selected = X_train[top_features]\n",
        "    X_test_selected = X_test[top_features]\n",
        "\n",
        "    # Standardize the features (important for Logistic Regression)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_selected)\n",
        "    X_test_scaled = scaler.transform(X_test_selected)\n",
        "\n",
        "    # Check if y_train contains at least 2 classes\n",
        "    if len(np.unique(y_train)) < 2:\n",
        "        print(f\"Warning: y_train contains only one class for num_features = {num_features}. Skipping Logistic Regression.\")\n",
        "        continue  # Skip to the next iteration\n",
        "\n",
        "    # Train Logistic Regression classifier\n",
        "    lr_clf = LogisticRegression(solver=\"liblinear\", C=1.0, random_state=42)\n",
        "    lr_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = lr_clf.predict(X_test_scaled)\n",
        "    y_pred_proba = lr_clf.predict_proba(X_test_scaled)[:, 1]  # Probability estimates for AUC calculation\n",
        "\n",
        "    # Evaluate model performance\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "    # Store results in DataFrame\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([[num_features, auc, accuracy, precision, kappa]],\n",
        "                                                      columns=[\"Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])],\n",
        "                                                      ignore_index=True)\n",
        "\n",
        "# Displaying final results\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmTl3UE9rVfo",
        "outputId": "1f560889-5414-4e2e-c4e0-d508251f322c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [01:13:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "<ipython-input-11-f8dc8f20b011>:43: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  results_df = pd.concat([results_df, pd.DataFrame([[num_features, auc, accuracy, precision, kappa]],\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Features       AUC  Accuracy  Precision     Kappa\n",
            "0       10  0.975673  0.928058   0.953846  0.856048\n",
            "1       50  0.988095  0.949640   0.955882  0.899265\n",
            "2      100  0.988820  0.964029   0.984848  0.928031\n",
            "3      500  0.987578  0.949640   0.955882  0.899265\n",
            "4     1000  0.987371  0.949640   0.955882  0.899265\n"
          ]
        }
      ],
      "source": [
        "#XGB + RF\n",
        "# Train XGBoost to get feature importances\n",
        "xgb_selector = XGBClassifier(n_estimators=500, learning_rate=0.05, max_depth=6,\n",
        "                             subsample=0.8, colsample_bytree=0.8, random_state=42,\n",
        "                             use_label_encoder=False, eval_metric=\"logloss\")\n",
        "\n",
        "xgb_selector.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = pd.Series(xgb_selector.feature_importances_, index=X.columns)\n",
        "\n",
        "# List of feature selection counts\n",
        "feature_counts = [10, 50, 100, 500, 1000]\n",
        "\n",
        "# Initialize a DataFrame to store results\n",
        "results_df = pd.DataFrame(columns=[\"Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])\n",
        "\n",
        "# Loop through different feature counts\n",
        "for num_features in feature_counts:\n",
        "    # Select top N features\n",
        "    top_features = feature_importances.nlargest(num_features).index.tolist()\n",
        "\n",
        "    # Filter dataset with selected features\n",
        "    X_train_selected = X_train[top_features]\n",
        "    X_test_selected = X_test[top_features]\n",
        "\n",
        "    # Train Random Forest classifier\n",
        "    rf_clf = RandomForestClassifier(n_estimators=500, max_depth=None, min_samples_split=2,\n",
        "                                    random_state=42, n_jobs=-1)\n",
        "    rf_clf.fit(X_train_selected, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = rf_clf.predict(X_test_selected)\n",
        "    y_pred_proba = rf_clf.predict_proba(X_test_selected)[:, 1]  # Probability estimates for AUC calculation\n",
        "\n",
        "    # Evaluate model performance\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "    # Store results in DataFrame\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([[num_features, auc, accuracy, precision, kappa]],\n",
        "                                                      columns=[\"Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])],\n",
        "                                                      ignore_index=True)\n",
        "\n",
        "# Display final results\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H_lrUIFudMz",
        "outputId": "4abd937e-e919-4d8f-c3d6-753130ac2bcd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:52:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "<ipython-input-110-480001a55502>:43: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  results_df = pd.concat([results_df, pd.DataFrame([[threshold, num_features, auc, accuracy, precision, kappa]],\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:54:02] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:55:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:57:01] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:58:27] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Variance Threshold Num Features       AUC  Accuracy  Precision     Kappa\n",
            "0               0.001        31504  0.994372  0.982249   0.928571  0.886856\n",
            "1               0.005        31502  0.991991  0.976331   0.923077  0.844311\n",
            "2               0.010        31498  0.993074  0.982249   0.928571  0.886856\n",
            "3               0.050        31237  0.993290  0.976331   0.923077  0.844311\n",
            "4               0.100        30372  0.993290  0.982249   0.928571  0.886856\n"
          ]
        }
      ],
      "source": [
        "#Variance threshold + XGB\n",
        "# Define variance thresholds to experiment with\n",
        "variance_thresholds = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
        "\n",
        "# Initialize a DataFrame to store results\n",
        "results_df = pd.DataFrame(columns=[\"Variance Threshold\", \"Num Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])\n",
        "\n",
        "# Loop through different variance thresholds\n",
        "for threshold in variance_thresholds:\n",
        "    # Apply VarianceThreshold feature selection\n",
        "    selector = VarianceThreshold(threshold=threshold)\n",
        "    X_train_selected = selector.fit_transform(X_train)\n",
        "    X_test_selected = selector.transform(X_test)\n",
        "\n",
        "    # Get selected feature names\n",
        "    selected_features = X.columns[selector.get_support()]\n",
        "    num_features = len(selected_features)\n",
        "\n",
        "    # Train XGBoost classifier\n",
        "    xgb_clf = XGBClassifier(\n",
        "        n_estimators=500,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=6,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric=\"logloss\"\n",
        "    )\n",
        "    xgb_clf.fit(X_train_selected, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = xgb_clf.predict(X_test_selected)\n",
        "    y_pred_proba = xgb_clf.predict_proba(X_test_selected)[:, 1]  # Probability estimates for AUC calculation\n",
        "\n",
        "    # Evaluate model performance\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "    # Store results in DataFrame\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([[threshold, num_features, auc, accuracy, precision, kappa]],\n",
        "                                                      columns=[\"Variance Threshold\", \"Num Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])],\n",
        "                                                      ignore_index=True)\n",
        "\n",
        "# Display final results\n",
        "print(results_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xq7gPPsFvBeZ",
        "outputId": "c33f45be-4272-4f44-d6f7-1e766af0b685"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-109-4c8ce2bce1cc>:39: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  results_df = pd.concat([results_df, pd.DataFrame([[threshold, num_features, auc, accuracy, precision, kappa]],\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Variance Threshold Num Features       AUC  Accuracy  Precision     Kappa\n",
            "0               0.001        31504  0.995238  0.994083     0.9375  0.964488\n",
            "1               0.005        31502  0.995238  0.994083     0.9375  0.964488\n",
            "2               0.010        31498  0.995238  0.994083     0.9375  0.964488\n",
            "3               0.050        31237  0.995238  0.994083     0.9375  0.964488\n",
            "4               0.100        30372  0.995238  0.994083     0.9375  0.964488\n"
          ]
        }
      ],
      "source": [
        "#Variance Threshold + SVM\n",
        "# Define variance thresholds to experiment with\n",
        "variance_thresholds = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
        "\n",
        "# Initialize a DataFrame to store results\n",
        "results_df = pd.DataFrame(columns=[\"Variance Threshold\", \"Num Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])\n",
        "\n",
        "# Loop through different variance thresholds\n",
        "for threshold in variance_thresholds:\n",
        "    # Apply VarianceThreshold feature selection\n",
        "    selector = VarianceThreshold(threshold=threshold)\n",
        "    X_train_selected = selector.fit_transform(X_train)\n",
        "    X_test_selected = selector.transform(X_test)\n",
        "\n",
        "    # Get selected feature names\n",
        "    selected_features = X.columns[selector.get_support()]\n",
        "    num_features = len(selected_features)\n",
        "\n",
        "    # Standardize the features (SVM is sensitive to scaling)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_selected)\n",
        "    X_test_scaled = scaler.transform(X_test_selected)\n",
        "\n",
        "    # Train SVM classifier\n",
        "    svm_clf = SVC(kernel=\"rbf\", probability=True, C=1, gamma=\"scale\", random_state=42)\n",
        "    svm_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = svm_clf.predict(X_test_scaled)\n",
        "    y_pred_proba = svm_clf.predict_proba(X_test_scaled)[:, 1]  # Probability estimates for AUC calculation\n",
        "\n",
        "    # Evaluate model performance\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "    # Store results in DataFrame\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([[threshold, num_features, auc, accuracy, precision, kappa]],\n",
        "                                                      columns=[\"Variance Threshold\", \"Num Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])],\n",
        "                                                      ignore_index=True)\n",
        "\n",
        "# Display final results\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9RHcm2xyCcU",
        "outputId": "3e9896da-c7d8-430c-c830-523cb271c028"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-108-fc1b02ff988e>:39: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  results_df = pd.concat([results_df, pd.DataFrame([[threshold, num_features, auc, accuracy, precision, kappa]],\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Variance Threshold Num Features       AUC  Accuracy  Precision     Kappa\n",
            "0               0.001        31504  0.996537  0.934911   0.576923  0.697674\n",
            "1               0.005        31502  0.996537  0.934911   0.576923  0.697674\n",
            "2               0.010        31498  0.996537  0.934911   0.576923  0.697674\n",
            "3               0.050        31237  0.996537  0.934911   0.576923  0.697674\n",
            "4               0.100        30372  0.996537  0.934911   0.576923  0.697674\n"
          ]
        }
      ],
      "source": [
        "#Variance threshold + LR\n",
        "\n",
        "# Define variance thresholds to test\n",
        "variance_thresholds = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
        "\n",
        "# Initialize a DataFrame to store results\n",
        "results_df = pd.DataFrame(columns=[\"Variance Threshold\", \"Num Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])\n",
        "\n",
        "# Loop through different variance thresholds\n",
        "for threshold in variance_thresholds:\n",
        "    # Apply VarianceThreshold feature selection\n",
        "    selector = VarianceThreshold(threshold=threshold)\n",
        "    X_train_selected = selector.fit_transform(X_train)\n",
        "    X_test_selected = selector.transform(X_test)\n",
        "\n",
        "    # Get the number of selected features\n",
        "    num_features = X_train_selected.shape[1]\n",
        "\n",
        "    # Standardize the features (LR benefits from scaling)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_selected)\n",
        "    X_test_scaled = scaler.transform(X_test_selected)\n",
        "\n",
        "    # Train Logistic Regression model\n",
        "    lr_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
        "    lr_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = lr_clf.predict(X_test_scaled)\n",
        "    y_pred_proba = lr_clf.predict_proba(X_test_scaled)[:, 1]  # Probability estimates for AUC calculation\n",
        "\n",
        "    # Evaluate model performance\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "    # Store results in DataFrame\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([[threshold, num_features, auc, accuracy, precision, kappa]],\n",
        "                                                      columns=[\"Variance Threshold\", \"Num Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])],\n",
        "                                                      ignore_index=True)\n",
        "\n",
        "# Display final results\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGzgnRriyLuS",
        "outputId": "6fbc7692-6ea7-47c4-9928-2d86d693ce52"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-107-5419cb2f25dc>:39: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  results_df = pd.concat([results_df, pd.DataFrame([[threshold, num_features, auc, accuracy, precision, kappa]],\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Variance Threshold Num Features       AUC  Accuracy  Precision     Kappa\n",
            "0               0.001        31504  0.993506  0.994083   0.937500  0.964488\n",
            "1               0.005        31502  0.993939  0.994083   0.937500  0.964488\n",
            "2               0.010        31498  0.994372  0.988166   0.933333  0.926840\n",
            "3               0.050        31237  0.994805  0.994083   0.937500  0.964488\n",
            "4               0.100        30372  0.995238  0.994083   0.937500  0.964488\n"
          ]
        }
      ],
      "source": [
        "#Variance treshold + RF\n",
        "# Define variance thresholds to test\n",
        "variance_thresholds = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
        "\n",
        "# Initialize a DataFrame to store results\n",
        "results_df = pd.DataFrame(columns=[\"Variance Threshold\", \"Num Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])\n",
        "\n",
        "# Loop through different variance thresholds\n",
        "for threshold in variance_thresholds:\n",
        "    # Apply VarianceThreshold feature selection\n",
        "    selector = VarianceThreshold(threshold=threshold)\n",
        "    X_train_selected = selector.fit_transform(X_train)\n",
        "    X_test_selected = selector.transform(X_test)\n",
        "\n",
        "    # Get the number of selected features\n",
        "    num_features = X_train_selected.shape[1]\n",
        "\n",
        "    # Train Random Forest classifier\n",
        "    rf_clf = RandomForestClassifier(\n",
        "        n_estimators=200,  # More trees for better learning\n",
        "        max_depth=None,  # Let it grow deep\n",
        "        min_samples_split=2,  # Standard split settings\n",
        "        n_jobs=-1,  # Use all processors\n",
        "        random_state=42\n",
        "    )\n",
        "    rf_clf.fit(X_train_selected, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = rf_clf.predict(X_test_selected)\n",
        "    y_pred_proba = rf_clf.predict_proba(X_test_selected)[:, 1]  # Probability estimates for AUC calculation\n",
        "\n",
        "    # Evaluate model performance\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "    # Store results in DataFrame\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([[threshold, num_features, auc, accuracy, precision, kappa]],\n",
        "                                                      columns=[\"Variance Threshold\", \"Num Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])],\n",
        "                                                      ignore_index=True)\n",
        "\n",
        "# Display final results\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtP7wPrT3jvy",
        "outputId": "c2dca1af-e028-4daa-c28e-79172ce51d58"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:50:23] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "<ipython-input-106-42cd4b9bcb6e>:50: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  results_df = pd.concat([results_df, pd.DataFrame([[num_features, auc, accuracy, precision, kappa]],\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:50:23] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:50:23] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:50:23] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:50:25] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Num Features       AUC  Accuracy  Precision     Kappa\n",
            "0           10  0.995455  0.976331   0.923077  0.844311\n",
            "1           50  0.995238  0.982249   0.928571  0.886856\n",
            "2          100  0.994805  0.982249   0.928571  0.886856\n",
            "3          500  0.995022  0.982249   0.928571  0.886856\n",
            "4         1000  0.994589  0.982249   0.928571  0.886856\n"
          ]
        }
      ],
      "source": [
        "#Mutual info + XGB\n",
        "# Define feature selection sizes\n",
        "feature_counts = [10, 50, 100, 500, 1000]\n",
        "\n",
        "# Compute Mutual Information scores\n",
        "mi_scores = mutual_info_classif(X_train, y_train, random_state=42)\n",
        "mi_series = pd.Series(mi_scores, index=X_train.columns)\n",
        "\n",
        "# Initialize a DataFrame to store results\n",
        "results_df = pd.DataFrame(columns=[\"Num Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])\n",
        "\n",
        "# Loop through different feature counts\n",
        "for num_features in feature_counts:\n",
        "    # Select top N features\n",
        "    top_features = mi_series.nlargest(num_features).index.tolist()\n",
        "\n",
        "    # Filter dataset with selected features\n",
        "    X_train_selected = X_train[top_features]\n",
        "    X_test_selected = X_test[top_features]\n",
        "\n",
        "    # Standardize the features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_selected)\n",
        "    X_test_scaled = scaler.transform(X_test_selected)\n",
        "\n",
        "    # Train XGBoost model with optimized settings\n",
        "    xgb_clf = XGBClassifier(\n",
        "        n_estimators=500,  # More trees for better learning\n",
        "        learning_rate=0.05,  # Slower learning for better generalization\n",
        "        max_depth=6,  # Optimal depth to prevent overfitting\n",
        "        subsample=0.8,  # Helps prevent overfitting\n",
        "        colsample_bytree=0.8,  # Randomly selects features for better generalization\n",
        "        random_state=42,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric=\"logloss\"\n",
        "    )\n",
        "    xgb_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = xgb_clf.predict(X_test_scaled)\n",
        "    y_pred_proba = xgb_clf.predict_proba(X_test_scaled)[:, 1]  # Probability estimates for AUC calculation\n",
        "\n",
        "    # Evaluate model performance\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "    # Store results in DataFrame\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([[num_features, auc, accuracy, precision, kappa]],\n",
        "                                                      columns=[\"Num Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])],\n",
        "                                                      ignore_index=True)\n",
        "\n",
        "# Display final results\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zi5dzPZn3rtp",
        "outputId": "10e7488f-350f-44c8-ab0d-7f818e94e56e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-105-0b8ffb9ddd33>:46: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  results_df = pd.concat([results_df, pd.DataFrame([[num_features, auc, accuracy, precision, kappa]],\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Num Features       AUC  Accuracy  Precision     Kappa\n",
            "0           10  0.996537  0.988166   0.933333  0.926840\n",
            "1           50  0.994805  0.994083   0.937500  0.964488\n",
            "2          100  0.994805  0.988166   0.933333  0.926840\n",
            "3          500  0.994805  0.994083   0.937500  0.964488\n",
            "4         1000  0.994805  0.994083   0.937500  0.964488\n"
          ]
        }
      ],
      "source": [
        "#Mutual info + SVM\n",
        "# Define feature selection sizes\n",
        "feature_counts = [10, 50, 100, 500, 1000]\n",
        "\n",
        "# Compute Mutual Information scores\n",
        "mi_scores = mutual_info_classif(X_train, y_train, random_state=42)\n",
        "mi_series = pd.Series(mi_scores, index=X_train.columns)\n",
        "\n",
        "# Initialize a DataFrame to store results\n",
        "results_df = pd.DataFrame(columns=[\"Num Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])\n",
        "\n",
        "# Loop through different feature counts\n",
        "for num_features in feature_counts:\n",
        "    # Select top N features\n",
        "    top_features = mi_series.nlargest(num_features).index.tolist()\n",
        "\n",
        "    # Filter dataset with selected features\n",
        "    X_train_selected = X_train[top_features]\n",
        "    X_test_selected = X_test[top_features]\n",
        "\n",
        "    # Standardize the features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_selected)\n",
        "    X_test_scaled = scaler.transform(X_test_selected)\n",
        "\n",
        "    # Train SVM model with optimized settings\n",
        "    svm_clf = SVC(\n",
        "        kernel=\"rbf\",  # RBF kernel for non-linearity\n",
        "        C=1.0,  # Regularization strength\n",
        "        probability=True,  # Enable probability estimates for AUC calculation\n",
        "        random_state=42\n",
        "    )\n",
        "    svm_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = svm_clf.predict(X_test_scaled)\n",
        "    y_pred_proba = svm_clf.predict_proba(X_test_scaled)[:, 1]  # Probability estimates for AUC calculation\n",
        "\n",
        "    # Evaluate model performance\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "    # Store results in DataFrame\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([[num_features, auc, accuracy, precision, kappa]],\n",
        "                                                      columns=[\"Num Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])],\n",
        "                                                      ignore_index=True)\n",
        "\n",
        "# Display final results\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCoHBMeO3vN3",
        "outputId": "ce110c4e-6dec-472c-f25c-0b87c6b2c689"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-104-7341fc600d6d>:46: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  results_df = pd.concat([results_df, pd.DataFrame([[num_features, auc, accuracy, precision, kappa]],\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Num Features       AUC  Accuracy  Precision     Kappa\n",
            "0           10  0.995238  0.988166   0.933333  0.926840\n",
            "1           50  0.996537  0.988166   0.882353  0.930992\n",
            "2          100  0.995238  0.988166   0.882353  0.930992\n",
            "3          500  0.995671  0.988166   0.882353  0.930992\n",
            "4         1000  0.996970  0.988166   0.882353  0.930992\n"
          ]
        }
      ],
      "source": [
        "#Mutual info + LR\n",
        "# Define feature selection sizes\n",
        "feature_counts = [10, 50, 100, 500, 1000]\n",
        "\n",
        "# Compute Mutual Information scores\n",
        "mi_scores = mutual_info_classif(X_train, y_train, random_state=42)\n",
        "mi_series = pd.Series(mi_scores, index=X_train.columns)\n",
        "\n",
        "# Initialize a DataFrame to store results\n",
        "results_df = pd.DataFrame(columns=[\"Num Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])\n",
        "\n",
        "# Loop through different feature counts\n",
        "for num_features in feature_counts:\n",
        "    # Select top N features\n",
        "    top_features = mi_series.nlargest(num_features).index.tolist()\n",
        "\n",
        "    # Filter dataset with selected features\n",
        "    X_train_selected = X_train[top_features]\n",
        "    X_test_selected = X_test[top_features]\n",
        "\n",
        "    # Standardize the features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_selected)\n",
        "    X_test_scaled = scaler.transform(X_test_selected)\n",
        "\n",
        "    # Train Logistic Regression model with optimized settings\n",
        "    lr_clf = LogisticRegression(\n",
        "        penalty=\"l2\",  # Ridge regularization\n",
        "        C=1.0,  # Regularization strength\n",
        "        solver=\"liblinear\",  # Good for small datasets\n",
        "        random_state=42\n",
        "    )\n",
        "    lr_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = lr_clf.predict(X_test_scaled)\n",
        "    y_pred_proba = lr_clf.predict_proba(X_test_scaled)[:, 1]  # Probability estimates for AUC calculation\n",
        "\n",
        "    # Evaluate model performance\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "    # Store results in DataFrame\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([[num_features, auc, accuracy, precision, kappa]],\n",
        "                                                      columns=[\"Num Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])],\n",
        "                                                      ignore_index=True)\n",
        "\n",
        "# Display final results\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQkG4qFL31yc",
        "outputId": "900cee09-b46d-48cd-8774-d8dd372b3360"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-103-1acac8c4c5ea>:49: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  results_df = pd.concat([results_df, pd.DataFrame([[num_features, auc, accuracy, precision, kappa]],\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Num Features       AUC  Accuracy  Precision     Kappa\n",
            "0           10  0.995238  0.982249   0.928571  0.886856\n",
            "1           50  0.994372  0.988166   0.933333  0.926840\n",
            "2          100  0.995022  0.988166   0.933333  0.926840\n",
            "3          500  0.994805  0.988166   0.933333  0.926840\n",
            "4         1000  0.994805  0.988166   0.933333  0.926840\n"
          ]
        }
      ],
      "source": [
        "#Mutual info + RF\n",
        "# Define feature selection sizes\n",
        "feature_counts = [10, 50, 100, 500, 1000]\n",
        "\n",
        "# Compute Mutual Information scores\n",
        "mi_scores = mutual_info_classif(X_train, y_train, random_state=42)\n",
        "mi_series = pd.Series(mi_scores, index=X_train.columns)\n",
        "\n",
        "# Initialize a DataFrame to store results\n",
        "results_df = pd.DataFrame(columns=[\"Num Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])\n",
        "\n",
        "# Loop through different feature counts\n",
        "for num_features in feature_counts:\n",
        "    # Select top N features\n",
        "    top_features = mi_series.nlargest(num_features).index.tolist()\n",
        "\n",
        "    # Filter dataset with selected features\n",
        "    X_train_selected = X_train[top_features]\n",
        "    X_test_selected = X_test[top_features]\n",
        "\n",
        "    # Standardize the features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_selected)\n",
        "    X_test_scaled = scaler.transform(X_test_selected)\n",
        "\n",
        "    # Train Random Forest model with optimized settings\n",
        "    rf_clf = RandomForestClassifier(\n",
        "        n_estimators=500,  # More trees for better learning\n",
        "        max_depth=None,  # Let trees grow fully\n",
        "        min_samples_split=2,  # Default split setting\n",
        "        min_samples_leaf=1,  # Small leaf size for more splits\n",
        "        bootstrap=True,  # Bootstrap sampling\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    rf_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = rf_clf.predict(X_test_scaled)\n",
        "    y_pred_proba = rf_clf.predict_proba(X_test_scaled)[:, 1]  # Probability estimates for AUC calculation\n",
        "\n",
        "    # Evaluate model performance\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "    # Store results in DataFrame\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([[num_features, auc, accuracy, precision, kappa]],\n",
        "                                                      columns=[\"Num Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])],\n",
        "                                                      ignore_index=True)\n",
        "\n",
        "# Display final results\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jgex3ZUk38YU",
        "outputId": "b7e3cd86-c525-4b6c-e81c-f068a7d53086"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:26:54] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:31:06] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "<ipython-input-4-535f5595d775>:54: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  results_df = pd.concat([results_df, pd.DataFrame([[num_features, auc, accuracy, precision, kappa]],\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:31:07] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:31:07] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:31:10] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:31:14] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Num Features       AUC  Accuracy  Precision     Kappa\n",
            "0           10  0.971843  0.920863   0.914286  0.841735\n",
            "1           50  0.989234  0.942446   0.955224  0.884862\n",
            "2          100  0.991925  0.942446   0.955224  0.884862\n",
            "3          500  0.989441  0.942446   0.942029  0.884886\n",
            "4         1000  0.987992  0.928058   0.927536  0.856108\n"
          ]
        }
      ],
      "source": [
        "#XGB + XGB\n",
        "from sklearn.preprocessing import StandardScaler # Importing the necessary class\n",
        "# Define feature selection sizes\n",
        "feature_counts = [10, 50, 100, 500, 1000]\n",
        "\n",
        "# Train XGBoost model for feature selection\n",
        "xgb_selector = XGBClassifier(n_estimators=500, learning_rate=0.05, max_depth=6, subsample=0.8, colsample_bytree=0.8, random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\n",
        "xgb_selector.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = pd.Series(xgb_selector.feature_importances_, index=X_train.columns)\n",
        "\n",
        "# Initialize a DataFrame to store results\n",
        "results_df = pd.DataFrame(columns=[\"Num Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])\n",
        "\n",
        "# Loop through different feature counts\n",
        "for num_features in feature_counts:\n",
        "    # Select top N features\n",
        "    top_features = feature_importances.nlargest(num_features).index.tolist()\n",
        "\n",
        "    # Filter dataset with selected features\n",
        "    X_train_selected = X_train[top_features]\n",
        "    X_test_selected = X_test[top_features]\n",
        "\n",
        "    # Standardize the features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_selected)\n",
        "    X_test_scaled = scaler.transform(X_test_selected)\n",
        "\n",
        "    # Train XGBoost model with optimized settings\n",
        "    xgb_clf = XGBClassifier(\n",
        "        n_estimators=500,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=6,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric=\"logloss\"\n",
        "    )\n",
        "    xgb_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = xgb_clf.predict(X_test_scaled)\n",
        "    y_pred_proba = xgb_clf.predict_proba(X_test_scaled)[:, 1]  # Probability estimates for AUC calculation\n",
        "\n",
        "    # Evaluate model performance\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "    # Store results in DataFrame\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([[num_features, auc, accuracy, precision, kappa]],\n",
        "                                                      columns=[\"Num Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])],\n",
        "                                                      ignore_index=True)\n",
        "\n",
        "# Display final results\n",
        "print(results_df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
